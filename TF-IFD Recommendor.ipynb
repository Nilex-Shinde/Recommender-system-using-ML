{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\miniconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "###Install requirements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "data = pd.read_csv('sample-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def clean(Text):\n",
    "    Text = re.sub('-',' ',Text)\n",
    "    Text = re.sub('>','> ',Text)\n",
    "    Text = re.sub('<',' <',Text)\n",
    "    Text = \"\".join([ch for ch in Text if ch not in string.punctuation])\n",
    "    cleanr = re.compile('<.*>')\n",
    "    cleantext = re.sub(cleanr, ' ', Text)\n",
    "    cleantext = re.sub(' +',' ',cleantext)\n",
    "    return cleantext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## very little cleaning\n",
    "data_clean = data.copy()\n",
    "data_clean['description'] = data_clean['description'].str.lower()\n",
    "data_clean['description'] = data_clean['description'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 1:\n",
    "## We first try the statistical NLP approach by calculating the tf-idf based features for the dataset\n",
    "## and then use the cosine similarity function to calculate the list of most similar products to a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a TF-IDF matrix of unigrams, bigrams, and trigrams for each product. The 'stop_words' param\n",
    "## tells the TF-IDF module to ignore common english words like 'the', etc.\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(data_clean['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_m(product_id,cosine_similarities,data_clean,number_similar_products =10,show_simlar = 2):\n",
    "    similar_products = cosine_similarities[product_id-1].argsort()[:-number_similar_products:-1]\n",
    "    similar_products = similar_products[1:,]\n",
    "    print('Description of Product:')\n",
    "    print(data[\"description\"][product_id-1])\n",
    "    print('\\n')\n",
    "    print(\"Similar product to {0} : \".format(product_id))\n",
    "    for x in similar_products:\n",
    "        print(x+1)\n",
    "    print('\\n')\n",
    "    print('Description of Most Similar Product:')\n",
    "    print(data[\"description\"][similar_products[0]])\n",
    "    print('\\n')\n",
    "    print(data[\"description\"][similar_products[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description of Product:\n",
      "Baby micro d-luxe cardigan - Micro D-Luxe is a heavenly soft fabric with down-to-earth applications. This cardigan is made from a quick-drying, durable 4.6-oz 100% polyester (87% recycled) microdenier fleece that is lightweight and breathable so it can work as a top or midlayer. A wind flap backs the zip front, while the hood has covered-elastic side trim to bundle babies in deep warmth. With slash-style patch pockets, color-coordinating elbow patches and soft self-fabric cuffs. Recyclable through the Common Threads Recycling Program.<br><br><b>Details:</b><ul> <li>\"Soft, microdenier fleece made from 87% recycled polyester is durable and quick-to-dry\"</li> <li>Hood stays secure with covered side elastic</li> <li>Inside neck tape for a clean finish</li> <li>Handwarmer patch pockets</li> <li>\"Full front zip backed by a wind flap; soft, self-fabric cuffs\"</li></ul><br><br><b>Fabric: </b>4.6-oz 100% microdenier polyester (87% recycled) fleece. Recyclable through the Common Threads Recycling Program<br><br><b>Weight: </b>(121 g 4.2 oz)<br><br>Made in Costa Rica.\n",
      "\n",
      "\n",
      "Similar product to 9 : \n",
      "417\n",
      "469\n",
      "474\n",
      "475\n",
      "230\n",
      "414\n",
      "460\n",
      "461\n",
      "\n",
      "\n",
      "Description of Most Similar Product:\n",
      "Micro d-luxe cardigan - A Micro D-Luxe Cardigan adds irresistible warmth to top-roping Lisa Falls, riding a section of the White Rim Trail, or just watching campfire shadows through half-closed eyes. Made from lightweight 4.1-oz 100% polyester (87% recycled) microdenier fleece that's quick to dry, durable and breathable, with features that include a deep 3-panel hood, chafe-free seamless shoulders and handwarmer pockets. Recyclable through the Common Threads Recycling Program.<br><br><b>Details:</b><ul> <li>\"Soft, microdenier fleece is durable, quick-to-dry and made from 87% recycled polyester\"</li> <li>3-piece contoured hood</li> <li>Seamless shoulders to prevent chafe under backpack straps</li> <li>Handwarmer pockets</li></ul><br><br><b>Fabric: </b>4.1-oz 100% microdenier polyester (87% recycled) fleece. Recyclable through the Common Threads Recycling Program<br><br><b>Weight: </b>(213 g 7.4 oz)<br><br>Made in Costa Rica.\n",
      "\n",
      "\n",
      "Baby duality jkt - Reversibility and three-season versatility are virtues of our Baby Duality Jacket. The wind-resistant shell side is made from a 2.3-oz 100% all-recycled polyester and treated with a Deluge DWR (durable water repellent) finish to resist wet weather; the breathable 4.4-oz 100% all-recycled polyester microdenier fleece side provides fuzzy warmth. The Duality has a protective chin flap and zipper garage; spandex binding on the hood, cuffs and waist seals out the chill. With fleece-side handwarmer pockets, elbow patches and a reflective logo, and shell-side reflective zipper and reflective logo. Recyclable through the Common Threads Recycling Program<br><br><b>Details:</b><ul> <li>Reverses from a water-and-wind resistant Deluge DWR (durable water repellent) treated polyester shell side to a cozy microdenier fleece side</li> <li>Fleece side has handwarmer pockets and elbow patches</li> <li>\"Spandex binding on cuffs, waist and hood holds shape and seals out cold\"</li> <li>Reflective logo on both sides and reflective zipper on shell side</li></ul><br><br><b>Fabric: </b>Shell: 2.3-oz 100% all-recycled polyester with a Deluge DWR (durable water repellent) finish. Fleece: 4.4-oz 100% all-recycled polyester. Recyclable through the Common Threads Recycling Program<br><br><b>Weight: </b>(204 g 7.1 oz)<br><br>Made in China.\n"
     ]
    }
   ],
   "source": [
    "get_similarity_m(9,cosine_similarities,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPROACH 2:\n",
    "## We use word 2 vec to get the paragraph vector of the data\n",
    "## and then use the cosine similarity function to calculate the list of most similar products to a given input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(stop_words='english')\n",
    "vectorizer = tf_idf.fit(data_clean['description'])\n",
    "result = vectorizer.transform(data_clean['description'])\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = vectorizer.get_feature_names()\n",
    "result = np.array(result.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create dictionary with having tf-idf more than 0.1 and \n",
    "\n",
    "tf_idf_words = []\n",
    "for r in result:\n",
    "    dict_word = {}\n",
    "    non_zero = np.where(r != 0)[0]\n",
    "    for word in non_zero:\n",
    "        if r[word] > 0.1:\n",
    "            dict_word[names[word]] = r[word]\n",
    "    tf_idf_words.append(dict_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean['tf_idf_desc'] = tf_idf_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Install the Google word to vector #####\n",
    "os.system('''curl https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz -o GoogleNews-vectors-negative300.bin.gz && sudo gunzip GoogleNews-vectors-negative300.bin.gz ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Not enough space",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0df672e2a40d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##### load the available Google vectors ######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m        \u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GoogleNews-vectors-negative300.bin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1434\u001b[0m         return _load_word2vec_format(\n\u001b[0;32m   1435\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[0;32m   1437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\miniconda3\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m                     \u001b[0mch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb' '\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 12] Not enough space"
     ]
    }
   ],
   "source": [
    "##### load the available Google vectors ######\n",
    "model = gensim.models.KeyedVectors\\\n",
    "        .load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########  create the new word2vec #######\n",
    "\n",
    "def w2v_vectorize_desc(x):\n",
    "    coeff = 0\n",
    "    vector = np.zeros(300)\n",
    "    for word in x:\n",
    "        if word in model:\n",
    "            coeff = coeff + x[word]\n",
    "            vector = vector + model[word] * x[word]\n",
    "    if coeff != 0:\n",
    "        vector = vector / coeff\n",
    "    return list(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### apply the transformation #########\n",
    "data_clean['tf_idf_desc'] = data_clean\\\n",
    "            .apply(lambda x: w2v_vectorize_desc(x['tf_idf_desc']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v = data_clean[['id','tf_idf_desc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = data_w2v.id.values\n",
    "vectors = data_clean.tf_idf_desc.values\n",
    "\n",
    "file = open(\"product_glove\", \"w\")\n",
    "for p, v in zip(products, vectors):\n",
    "    vector_s = \" \".join(map(str, v))\n",
    "    file.write(str(p) + \" \" + vector_s + '\\n')\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### product_w2v  using gensim ###########\n",
    "glove2word2vec(\"product_glove\", \"product_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### load the model ##########\n",
    "product2vec = gensim.models.KeyedVectors.load_word2vec_format(\"product_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity_w2v(product_id,product2vec,data_clean,number_similar_products =10,show_simlar = 2):\n",
    "    similar_products = pd.DataFrame(product2vec.most_similar(str(product_id), topn = number_similar_products),columns= ['product_id','score'] )\n",
    "    print('Description of Product:')\n",
    "    print(data[\"description\"][product_id-1])\n",
    "    print('\\n')\n",
    "    print(\"Similar product to {0} : \".format(product_id))\n",
    "    for x in range(0,10):\n",
    "        print(similar_products.iloc[int(x),0])\n",
    "    print('\\n')\n",
    "    print('Description of Most Similar Product:')\n",
    "    print([data.loc[int(similar_products.iloc[1,0])-1,\"description\"]])\n",
    "    print('\\n')\n",
    "    print([data.loc[int(similar_products.iloc[2,0])-1,\"description\"]])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_similarity_w2v(45,product2vec,data_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
